import os, re, json, unicodedata
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126 Safari/537.36"
    ),
    "Accept-Language": "vi,en;q=0.9",
}

# -------------------------------
# ‚ú≥Ô∏è H√†m ti·ªán √≠ch
# -------------------------------
def normalize(s):
    """Chu·∫©n h√≥a chu·ªói ti·∫øng Vi·ªát, b·ªè d·∫•u & ch·ªØ hoa."""
    if not s:
        return ""
    s = unicodedata.normalize("NFKD", s)
    return "".join(c for c in s if not unicodedata.combining(c)).lower().strip()


def download_image(url, product_name, folder="images"):
    """T·∫£i ·∫£nh v·ªÅ th∆∞ m·ª•c images/ v√† tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô"""
    if not url or "http" not in url:
        return ""
    os.makedirs(folder, exist_ok=True)
    ext = os.path.splitext(url.split("?")[0])[-1] or ".jpg"
    safe_name = re.sub(r"[^\w\s-]", "", product_name).strip().replace(" ", "_")[:100]
    filename = f"{safe_name}{ext}"
    path = os.path.join(folder, filename)

    try:
        res = requests.get(url, headers=HEADERS, timeout=15)
        if res.status_code == 200:
            with open(path, "wb") as f:
                f.write(res.content)
            print(f"üì∏ ·∫¢nh ƒë√£ l∆∞u: {path}")
            return path.replace("\\", "/")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh {url}: {e}")
    return ""


# -------------------------------
# üîç T√°ch ƒë∆°n v·ªã t√≠nh + gi√° t·ª´ NEXT_DATA
# -------------------------------
def extract_units_from_next_data(data):
    """Duy·ªát ƒë·ªá quy __NEXT_DATA__ ƒë·ªÉ t√¨m c√°c ƒë∆°n v·ªã t√≠nh v√† gi√°"""
    result = []

    def walk(obj):
        if isinstance(obj, dict):
            # N·∫øu c√≥ tr∆∞·ªùng "units"
            if "units" in obj and isinstance(obj["units"], list):
                for u in obj["units"]:
                    unit_name = (
                        u.get("name")
                        or u.get("unit")
                        or u.get("unitName")
                        or ""
                    ).strip()

                    price = (
                        str(u.get("price") or u.get("salePrice") or "")
                        .replace("‚Ç´", "ƒë")
                        .strip()
                    )

                    if unit_name and re.search(r"\d[\d\.]*\s?[ƒë‚Ç´]", price):
                        p = (
                            re.search(r"\d[\d\.]*\s?[ƒë‚Ç´]", price)
                            .group(0)
                            .replace("‚Ç´", "ƒë")
                            .replace(" ", "")
                        )
                        result.append({"unit": unit_name, "price": p})

            # Duy·ªát s√¢u v√†o c√°c key c√≥ th·ªÉ ch·ª©a c·∫•u tr√∫c l·ªìng
            for key in ["variants", "skus", "items", "offers"]:
                if key in obj and isinstance(obj[key], (list, dict)):
                    walk(obj[key])

            for v in obj.values():
                walk(v)

        elif isinstance(obj, list):
            for v in obj:
                walk(v)

    walk(data)
    return result


# -------------------------------
# üß† L·∫•y chi ti·∫øt s·∫£n ph·∫©m Long Ch√¢u
# -------------------------------
def get_product_details(url):
    """L·∫•y th√¥ng tin s·∫£n ph·∫©m Long Ch√¢u ‚Äî ƒë·ªìng b·ªô v·ªõi MySQL"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=20)
        r.encoding = "utf-8"
        if r.status_code != 200:
            print(f"‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c: {url}")
            return None

        soup = BeautifulSoup(r.text, "html.parser")
        name = soup.find("h1").get_text(strip=True) if soup.find("h1") else ""

        # --- L·∫•y JSON g·ªëc t·ª´ __NEXT_DATA__ ---
        next_data_tag = soup.find("script", id="__NEXT_DATA__")
        units, base_price = [], ""
        if next_data_tag and next_data_tag.string:
            try:
                data = json.loads(next_data_tag.string)
                units = extract_units_from_next_data(data)
                offer_price = (
                    str(
                        data.get("props", {})
                        .get("pageProps", {})
                        .get("product", {})
                        .get("price", "")
                    ).strip()
                )
                base_price = offer_price + "ƒë" if offer_price else ""
            except Exception as e:
                print("‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c NEXT_DATA:", e)

        # --- Fallback gi√° ---
        if not base_price:
            price_tag = soup.find(text=re.compile(r"\d[\d\.]+[ƒë‚Ç´]"))
            base_price = price_tag.strip() if price_tag else ""

        # --- ·∫¢nh ---
        img_tag = soup.select_one(".swiper-slide img, .product-detail__thumb img")
        image = ""
        if img_tag:
            image = img_tag.get("src") or img_tag.get("data-src") or ""
            if image:
                image = urljoin(url, image)
        image_path = download_image(image, name)

        # --- Ti·ªán √≠ch l√†m s·∫°ch text ---
        def clean_html_value(val: str) -> str:
            if not val:
                return ""
            # b·ªè th·∫ª HTML, k√Ω t·ª± xu·ªëng d√≤ng v√† script Cloudflare
            val = re.sub(r"<[^>]+>", " ", val)
            val = re.sub(r"\(function\(.*?\}\)\(\)\;", " ", val)
            val = re.sub(r"\s+", " ", val)
            return val.strip()

        def find_val(label):
            t = soup.find(string=re.compile(label, re.I))
            if t:
                nxt = t.find_next()
                if nxt:
                    return clean_html_value(nxt.get_text(" ", strip=True))
            return ""

        brand = find_val("Th∆∞∆°ng hi·ªáu")
        category = find_val("Danh m·ª•c")
        registration = find_val("S·ªë ƒëƒÉng k√Ω")
        form = find_val("D·∫°ng b√†o ch·∫ø")
        size_spec = find_val("Quy c√°ch")
        manufacturer = find_val("Nh√† s·∫£n xu·∫•t")
        origin = find_val("Xu·∫•t x·ª©|N∆∞·ªõc s·∫£n xu·∫•t")
        ingredient = find_val("Th√†nh ph·∫ßn")

        # -------------------------
        # üí∞ L√†m s·∫°ch ƒë∆°n v·ªã & gi√°
        # -------------------------
        normalized_units = []
        for u in units:
            u_name = (u.get("unit") or "").strip()
            u_price = (u.get("price") or "").strip()
            if u_name and re.search(r"\d[\d\.]*\s?[ƒë‚Ç´]", u_price):
                p = (
                    re.search(r"\d[\d\.]*\s?[ƒë‚Ç´]", u_price)
                    .group(0)
                    .replace("‚Ç´", "ƒë")
                    .replace(" ", "")
                )
                normalized_units.append({"unit": u_name, "price": p})

        # N·∫øu kh√¥ng c√≥ ƒë∆°n v·ªã => m·∫∑c ƒë·ªãnh H·ªôp
        if not normalized_units and base_price:
            m = re.search(r"\d[\d\.]*\s?[ƒë‚Ç´]", base_price)
            if m:
                p = m.group(0).replace("‚Ç´", "ƒë").replace(" ", "")
                normalized_units = [{"unit": "H·ªôp", "price": p}]

        # Ch·ªçn ƒë∆°n v·ªã ƒë·∫ßu ti√™n l√†m gi√° hi·ªÉn th·ªã
        price = ""
        if normalized_units:
            price = f"{normalized_units[0]['price']} / {normalized_units[0]['unit']}"

        # N·∫øu kh√¥ng c√≥ t√™n ho·∫∑c gi√° => b·ªè qua
        if not price or not name:
            print(f"‚è© B·ªè qua {url}: kh√¥ng c√≥ t√™n ho·∫∑c gi√° h·ª£p l·ªá")
            return None

        # -------------------------
        # ‚úÖ Tr·∫£ v·ªÅ k·∫øt qu·∫£ s·∫°ch
        # -------------------------
        return {
            "url": url,
            "name": name,
            "price": price,
            "units": normalized_units,
            "brand": brand,
            "category": category,
            "registration": registration,
            "form": form,
            "size_spec": size_spec,
            "manufacturer": manufacturer,
            "origin": origin,
            "ingredient": ingredient,
            "image_path": image_path,
        }

    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω {url}: {e}")
        return None

